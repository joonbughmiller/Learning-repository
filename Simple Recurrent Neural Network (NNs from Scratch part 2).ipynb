{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "This project is a simple example of a \"many to one\" recurrent neural network that builds on knowledge from the simple neural network. This project will perform basic Sentiment Analysis and the tutorial for it can be found at:\n",
    "https://victorzhou.com/blog/intro-to-rnns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs can take variable-length sequences as both inputs and outputs - an advantage over vanilla neural networks and CNNs, which must have pre-determined sizes of inputs/outputs. RNNs work by iteratively updating a hidden state, which is a vector that can also have arbitrary dimension. At any given step t, the next hidden state is calculated using the previous hidden state and the next input x. It takes the same weights for each step (hence \"recurrent\").\n",
    "\n",
    "A vanilla RNN typically uses three sets of weights: input(t)->hidden layer, hidden layer(t-1)->hidden layer(t), and hidden layer(t)->output(t).\n",
    "\n",
    "We will use two biases: b(h) when calculating h(t) and b(y) when calculating y(t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each input will be a vector representing a word from the text.\n",
    "* Our chosen activation function will be the hyperbolic tangent (tanh) for calculating the hidden layer.\n",
    "* Our output will be a vector containing two numbers, one representing positive sentiments and the other negative. We will use Softmax to turn those into probabilities in order to ultimately decide the sentiment of the comment. Awesome reminder of how Softmax works: https://victorzhou.com/blog/softmax/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our data\n",
    "# Note: we are utilizing a small amount of data, so we will copy and paste our respective dictionaries.\n",
    "# If the dataset was larger, we would bring it in as a csv or other type of file.\n",
    "\n",
    "train_data = {\n",
    "  'good': True,\n",
    "  'bad': False,\n",
    "  'happy': True,\n",
    "  'sad': False,\n",
    "  'not good': False,\n",
    "  'not bad': True,\n",
    "  'not happy': False,\n",
    "  'not sad': True,\n",
    "  'very good': True,\n",
    "  'very bad': False,\n",
    "  'very happy': True,\n",
    "  'very sad': False,\n",
    "  'i am happy': True,\n",
    "  'this is good': True,\n",
    "  'i am bad': False,\n",
    "  'this is bad': False,\n",
    "  'i am sad': False,\n",
    "  'this is sad': False,\n",
    "  'i am not happy': False,\n",
    "  'this is not good': False,\n",
    "  'i am not bad': True,\n",
    "  'this is not sad': True,\n",
    "  'i am very happy': True,\n",
    "  'this is very good': True,\n",
    "  'i am very bad': False,\n",
    "  'this is very sad': False,\n",
    "  'this is very happy': True,\n",
    "  'i am good not bad': True,\n",
    "  'this is good not bad': True,\n",
    "  'i am bad not good': False,\n",
    "  'i am good and happy': True,\n",
    "  'this is not good and not happy': False,\n",
    "  'i am not at all good': False,\n",
    "  'i am not at all bad': True,\n",
    "  'i am not at all happy': False,\n",
    "  'this is not at all sad': True,\n",
    "  'this is not at all happy': False,\n",
    "  'i am good right now': True,\n",
    "  'i am bad right now': False,\n",
    "  'this is bad right now': False,\n",
    "  'i am sad right now': False,\n",
    "  'i was good earlier': True,\n",
    "  'i was happy earlier': True,\n",
    "  'i was bad earlier': False,\n",
    "  'i was sad earlier': False,\n",
    "  'i am very bad right now': False,\n",
    "  'this is very good right now': True,\n",
    "  'this is very sad right now': False,\n",
    "  'this was bad earlier': False,\n",
    "  'this was very good earlier': True,\n",
    "  'this was very bad earlier': False,\n",
    "  'this was very happy earlier': True,\n",
    "  'this was very sad earlier': False,\n",
    "  'i was good and not bad earlier': True,\n",
    "  'i was not good and not happy earlier': False,\n",
    "  'i am not at all bad or sad right now': True,\n",
    "  'i am not at all good or happy right now': False,\n",
    "  'this was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  'this is happy': True,\n",
    "  'i am good': True,\n",
    "  'this is not happy': False,\n",
    "  'i am not good': False,\n",
    "  'this is not bad': True,\n",
    "  'i am not sad': True,\n",
    "  'i am very good': True,\n",
    "  'this is very bad': False,\n",
    "  'i am very sad': False,\n",
    "  'this is bad not good': False,\n",
    "  'this is good and happy': True,\n",
    "  'i am not good and not happy': False,\n",
    "  'i am not at all sad': True,\n",
    "  'this is not at all good': False,\n",
    "  'this is not at all bad': True,\n",
    "  'this is good right now': True,\n",
    "  'this is sad right now': False,\n",
    "  'this is very bad right now': False,\n",
    "  'this was good earlier': True,\n",
    "  'i was not happy and not good earlier': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 unique words found\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary\n",
    "vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
    "vocab_size = len(vocab)\n",
    "print('%d unique words found' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "bad\n"
     ]
    }
   ],
   "source": [
    "# Now let's assign an integer index to represent each word in our vocab.\n",
    "word_to_idx = { w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = { i: w for i, w in enumerate(vocab)}\n",
    "print(word_to_idx['good'])\n",
    "print(idx_to_word[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one-hot vectors, which contain all zeros except for a single one, which represents the corresponding integer index. Thus, each x will be an 18-dimensional one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def createInputs(text):\n",
    "    \"\"\"\n",
    "    Returns an array of one-hot vectors representing the words in the input text string.\n",
    "    - text is a string\n",
    "    - Each one-hot vector has shape (vocab_size, 1)\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    for w in text.split(' '):\n",
    "        v = np.zeros((vocab_size, 1))\n",
    "        v[word_to_idx[w]] = 1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start implementing our RNN. We'll start by initializing the three weights and two biases it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50000273]\n",
      " [0.49999727]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import randn\n",
    "\n",
    "class RNN:\n",
    "    # A vanilla recurrent neural network\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = randn(hidden_size, hidden_size) / 1000 # We are dividing by 1000 to reduce initial variance of our weights. This is not the best way to initialize weights; we are using it for simplicity.\n",
    "        self.Wxh = randn(hidden_size, input_size) / 1000\n",
    "        self.Why = randn(output_size, hidden_size) / 1000\n",
    "        \n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the RNN using the given inputs\n",
    "        Returns the final output and hidden state.\n",
    "        - inputs is an array of one-hot vectors with shape (input_size, 1)\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "        \n",
    "        self.last_inputs = inputs # In tutorial, this line was inserted after creating the inputs\n",
    "        self.last_hs = { 0:h } # In tutorial, this line was inserted after creating the inputs\n",
    "        \n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.last_hs[i + 1] = h # In tutorial, this line was inserted after creating the inputs\n",
    "            \n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "        \n",
    "        return y, h\n",
    "    \n",
    "    def backprop(self, d_y, learn_rate=2e-2): # In tutorial, this definition was inserted after creating the inputs\n",
    "        \"\"\"\n",
    "        Perform a backward pass of the RNN.\n",
    "        - d_y (dL/dy) has shape (output_size, 1).\n",
    "        - learn_rate is a float.\n",
    "        \"\"\"\n",
    "        n = len(self.last_inputs)\n",
    "        \n",
    "        d_Why = d_y @ self.last_hs[n].T # In tutorial, this line was inserted after calculating derivatives\n",
    "        d_by = d_y # In tutorial, this line was inserted after calculating derivatives\n",
    "        \n",
    "        # Initialize dL/dWhh, dL/Wxh, and dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape) # In tutorial, this line was inserted after calculating derivatives\n",
    "        d_Wxh = np.zeros(self.Wxh.shape) # In tutorial, this line was inserted after calculating derivatives\n",
    "        d_bh = np.zeros(self.bh.shape) # In tutorial, this line was inserted after calculating derivatives\n",
    "        \n",
    "        # Calculate dL/dh for the last h.\n",
    "        d_h = self.Why.T @ d_y # In tutorial, this line was inserted after creating the inputs\n",
    "       \n",
    "# Now we need the gradients for W(hh), W(xh), and b(h). \n",
    "# To calculate the gradient for W(xh), we'll need to backpropagate through all timesteps \n",
    "# (Backpropagation Through Time - BPTT). We will add lines to `backprop` to do this.\n",
    "    \n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(n)): # In tutorial, this line was inserted after calculating derivatives\n",
    "            # An intermediate value: dL/dh * (1 - h^2)\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * d_h) # In tutorial, this line was inserted after calculating derivatives\n",
    "            \n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += temp # In tutorial, this line was inserted after calculating derivatives\n",
    "            \n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += temp @ self.last_hs[t].T # In tutorial, this line was inserted after calculating derivatives\n",
    "            \n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += temp @ self.last_inputs[t].T # In tutorial, this line was inserted after calculating derivatives\n",
    "            \n",
    "            # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = self.Whh @ temp # In tutorial, this line was inserted after calculating derivatives\n",
    "            \n",
    "            # Clip to prevent exploding gradients. This is when gradients become very large due to having lots of multiplied terms.\n",
    "            for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]: # In tutorial, this line was inserted after calculating derivatives\n",
    "                np.clip(d, -1, 1, out=d) # In tutorial, this line was inserted after calculating derivatives\n",
    "            \n",
    "            # Update weights and biases using gradient descent.\n",
    "            self.Whh -= learn_rate * d_Whh # In tutorial, this line was inserted after calculating derivatives\n",
    "            self.Wxh -= learn_rate * d_Wxh # In tutorial, this line was inserted after calculating derivatives\n",
    "            self.Why -= learn_rate * d_Why # In tutorial, this line was inserted after calculating derivatives\n",
    "            self.bh -= learn_rate * d_bh # In tutorial, this line was inserted after calculating derivatives\n",
    "            self.by -= learn_rate * d_by # In tutorial, this line was inserted after calculating derivatives\n",
    "            \n",
    "\n",
    "    \n",
    "def softmax(xs):\n",
    "    # Applies the Softmax Function to the input array\n",
    "    return np.exp(xs) / sum(np.exp(xs))\n",
    "    \n",
    "# Initialize our RNN!\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "inputs = createInputs('i am very good')\n",
    "out, h = rnn.forward(inputs)\n",
    "probs = softmax(out)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will begin working on the backward phase. To train our RNN, we will need a loss function. This example will use cross-entropy loss, which is often paired with Softmax and enables us to quantify how sure we are RNN has predicted correctly. Reminder tutorial here: https://victorzhou.com/blog/intro-to-cnns-part-1/#52-cross-entropy-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each training example\n",
    "for x, y in train_data.items():\n",
    "    inputs = createInputs(x)\n",
    "    target = int(y)\n",
    "    \n",
    "    # Forward\n",
    "    out, _ = rnn.forward(inputs)\n",
    "    probs = softmax(out)\n",
    "    \n",
    "    # Build dL/dy\n",
    "    d_L_d_y = probs\n",
    "    d_L_d_y[target] -= 1\n",
    "    \n",
    "    # Backward\n",
    "    rnn.backprop(d_L_d_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a helper function to process data with our RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def processData(data, backprop=True):\n",
    "    \"\"\"\n",
    "    Returns the RNN's loss and accuracy for the given data.\n",
    "    - data is a dictionary mapping text to True or False.\n",
    "    - backprop determines if the backward phase should be run.\n",
    "    \"\"\"\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "    \n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    for x, y in items:\n",
    "        inputs = createInputs(x)\n",
    "        target = int(y)\n",
    "        \n",
    "        # Forward\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        probs = softmax(out)\n",
    "        \n",
    "        # Calculate loss / accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        num_correct += int(np.argmax(probs) == target)\n",
    "        \n",
    "        if backprop:\n",
    "            # Build dL/dy\n",
    "            d_L_d_y = probs\n",
    "            d_L_d_y[target] -= 1\n",
    "            \n",
    "            # Backward\n",
    "            rnn.backprop(d_L_d_y)\n",
    "    return loss / len(data), num_correct / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! Now it's time to write our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 100\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 200\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 300\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 400\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 500\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 600\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 700\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 800\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 900\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n",
      "--- Epoch 1000\n",
      "Train:\tLoss 0.000 | Accuracy: 1.000\n",
      "Test:\tLoss 0.000 | Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# *Note: If we were using a large dataset that uses a lot of computing power,\n",
    "# we would want to write a callback bit here to stop the training once a threshold of improvement had been reached.\n",
    "for epoch in range(1000):\n",
    "    train_loss, train_acc = processData(train_data)\n",
    "    \n",
    "    if epoch % 100 == 99:\n",
    "        print('--- Epoch %d' % (epoch + 1))\n",
    "        print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "        \n",
    "        test_loss, test_acc = processData(test_data, backprop=False)\n",
    "        print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
