{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will be a simple, bare-bones feedforward neural network for learning purposes.\n",
    "\n",
    "Tutorial source:https://victorzhou.com/blog/intro-to-neural-networks/?fbclid=IwAR1nCgd2a1GzX1LvdrT_0cHn3Bl3xFJF7VUrmQw3j9CTMOwjfVGef52phBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our functions (sigmoid activation function, neuron, and feedforward)\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def feedforward(self, inputs):\n",
    "        # Weight inputs, add bias, then use the activation function\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "# Let's test with some sample numbers\n",
    "\n",
    "weights = np.array([0,1])\n",
    "bias = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])\n",
    "print(n.feedforward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Now let's define our Neural Network class\n",
    "class OurNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A neural network with\n",
    "        - two inputs\n",
    "        - a hidden layer with two neurons (h1, h2)\n",
    "        - an output layer with one neuron (o1)\n",
    "    Each neuron has the same weights and biase\n",
    "        - w = [0, 1]\n",
    "        - b = 0\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        weights = np.array([0, 1])\n",
    "        bias = 0\n",
    "        \n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        \n",
    "        # Inputs for o1 are the outputs from h1 and h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        \n",
    "        return out_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's train our neural network to predict someone's gender given their height and weight\n",
    "\n",
    "We have the following measurements:\n",
    "- Alice, 133, 65, F\n",
    "- Bob, 160, 72, M\n",
    "- Charlie, 152, 70, M\n",
    "- Diana, 120, 60, F\n",
    "\n",
    "Let's represent Male with 0 and Female with 1 and shift the data to make it easier to use. Normally you will shift by the mean.\n",
    "\n",
    "**Note:** With a larger dataset, we should read in our data, then write a function to make this conversion and possibly make an ID column instead of using names.\n",
    "\n",
    "- Alice, -2, -1, 1\n",
    "- Bob, 25, 6, 0\n",
    "- Charlie, 17, 4, 0\n",
    "- Diana, -15, -6, 1\n",
    "\n",
    "Before we train our network, let's define how to quantify how good it is. We will use the mean squared error (MSE) for our loss. It finds the difference between the correct answer and our model's predicted answer, squares that number, sums the squares, then divides by the number of samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Define MSE (loss)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length.\n",
    "    return((y_true - y_pred) **2).mean()\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1]) \n",
    "y_pred = np.array([0, 0, 0, 0]) # This array would occur if the model predicted all males\n",
    "\n",
    "\n",
    "print(mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training process will use stochastic gradient descent (we only operate on one sample at a time), which will tell us how to change our weights and biases to minimize loss.\n",
    "The process\n",
    "* 1 Choose one sample from dataset\n",
    "* 2 Calculate all the partial derivates of loss with respect to weights or biases.\n",
    "* 3 Use the update equation to update each weight and bias.\n",
    "* 4 Go back to step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Weights\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Biases\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # x is a numpy array with 2 elements.\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4* x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        \"\"\"\n",
    "        - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "        - all_y_trues is a numpy array with n elements.\n",
    "          Elements in all_y_trues correspond to those in data.\n",
    "          \"\"\"\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # ---Do a feedforward (we'll need these values later)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b2\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # --- Calculate partial derivatives\n",
    "                # --- naming represents \"partial L / partial w1\"\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                # --- Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "                # --- Calculate total loss at the end of each epoch\n",
    "                if epoch % 10 == 0:\n",
    "                    y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                    loss = mse_loss(all_y_trues, y_preds)\n",
    "                    print(\"Epoch %d loss: %.3f\" % (epoch, loss))         \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.526\n",
      "Epoch 0 loss: 0.526\n",
      "Epoch 0 loss: 0.525\n",
      "Epoch 0 loss: 0.522\n",
      "Epoch 10 loss: 0.422\n",
      "Epoch 10 loss: 0.422\n",
      "Epoch 10 loss: 0.422\n",
      "Epoch 10 loss: 0.415\n",
      "Epoch 20 loss: 0.298\n",
      "Epoch 20 loss: 0.298\n",
      "Epoch 20 loss: 0.297\n",
      "Epoch 20 loss: 0.292\n",
      "Epoch 30 loss: 0.223\n",
      "Epoch 30 loss: 0.222\n",
      "Epoch 30 loss: 0.221\n",
      "Epoch 30 loss: 0.219\n",
      "Epoch 40 loss: 0.175\n",
      "Epoch 40 loss: 0.173\n",
      "Epoch 40 loss: 0.172\n",
      "Epoch 40 loss: 0.171\n",
      "Epoch 50 loss: 0.138\n",
      "Epoch 50 loss: 0.137\n",
      "Epoch 50 loss: 0.136\n",
      "Epoch 50 loss: 0.136\n",
      "Epoch 60 loss: 0.111\n",
      "Epoch 60 loss: 0.110\n",
      "Epoch 60 loss: 0.109\n",
      "Epoch 60 loss: 0.109\n",
      "Epoch 70 loss: 0.090\n",
      "Epoch 70 loss: 0.089\n",
      "Epoch 70 loss: 0.089\n",
      "Epoch 70 loss: 0.088\n",
      "Epoch 80 loss: 0.074\n",
      "Epoch 80 loss: 0.074\n",
      "Epoch 80 loss: 0.073\n",
      "Epoch 80 loss: 0.073\n",
      "Epoch 90 loss: 0.062\n",
      "Epoch 90 loss: 0.062\n",
      "Epoch 90 loss: 0.062\n",
      "Epoch 90 loss: 0.062\n",
      "Epoch 100 loss: 0.053\n",
      "Epoch 100 loss: 0.053\n",
      "Epoch 100 loss: 0.053\n",
      "Epoch 100 loss: 0.053\n",
      "Epoch 110 loss: 0.046\n",
      "Epoch 110 loss: 0.046\n",
      "Epoch 110 loss: 0.046\n",
      "Epoch 110 loss: 0.046\n",
      "Epoch 120 loss: 0.040\n",
      "Epoch 120 loss: 0.040\n",
      "Epoch 120 loss: 0.040\n",
      "Epoch 120 loss: 0.040\n",
      "Epoch 130 loss: 0.036\n",
      "Epoch 130 loss: 0.036\n",
      "Epoch 130 loss: 0.035\n",
      "Epoch 130 loss: 0.035\n",
      "Epoch 140 loss: 0.032\n",
      "Epoch 140 loss: 0.032\n",
      "Epoch 140 loss: 0.032\n",
      "Epoch 140 loss: 0.032\n",
      "Epoch 150 loss: 0.029\n",
      "Epoch 150 loss: 0.029\n",
      "Epoch 150 loss: 0.029\n",
      "Epoch 150 loss: 0.029\n",
      "Epoch 160 loss: 0.026\n",
      "Epoch 160 loss: 0.026\n",
      "Epoch 160 loss: 0.026\n",
      "Epoch 160 loss: 0.026\n",
      "Epoch 170 loss: 0.024\n",
      "Epoch 170 loss: 0.024\n",
      "Epoch 170 loss: 0.024\n",
      "Epoch 170 loss: 0.024\n",
      "Epoch 180 loss: 0.022\n",
      "Epoch 180 loss: 0.022\n",
      "Epoch 180 loss: 0.022\n",
      "Epoch 180 loss: 0.022\n",
      "Epoch 190 loss: 0.020\n",
      "Epoch 190 loss: 0.020\n",
      "Epoch 190 loss: 0.020\n",
      "Epoch 190 loss: 0.020\n",
      "Epoch 200 loss: 0.019\n",
      "Epoch 200 loss: 0.019\n",
      "Epoch 200 loss: 0.019\n",
      "Epoch 200 loss: 0.019\n",
      "Epoch 210 loss: 0.018\n",
      "Epoch 210 loss: 0.018\n",
      "Epoch 210 loss: 0.017\n",
      "Epoch 210 loss: 0.017\n",
      "Epoch 220 loss: 0.016\n",
      "Epoch 220 loss: 0.016\n",
      "Epoch 220 loss: 0.016\n",
      "Epoch 220 loss: 0.016\n",
      "Epoch 230 loss: 0.015\n",
      "Epoch 230 loss: 0.015\n",
      "Epoch 230 loss: 0.015\n",
      "Epoch 230 loss: 0.015\n",
      "Epoch 240 loss: 0.015\n",
      "Epoch 240 loss: 0.015\n",
      "Epoch 240 loss: 0.014\n",
      "Epoch 240 loss: 0.014\n",
      "Epoch 250 loss: 0.014\n",
      "Epoch 250 loss: 0.014\n",
      "Epoch 250 loss: 0.014\n",
      "Epoch 250 loss: 0.014\n",
      "Epoch 260 loss: 0.013\n",
      "Epoch 260 loss: 0.013\n",
      "Epoch 260 loss: 0.013\n",
      "Epoch 260 loss: 0.013\n",
      "Epoch 270 loss: 0.012\n",
      "Epoch 270 loss: 0.012\n",
      "Epoch 270 loss: 0.012\n",
      "Epoch 270 loss: 0.012\n",
      "Epoch 280 loss: 0.012\n",
      "Epoch 280 loss: 0.012\n",
      "Epoch 280 loss: 0.012\n",
      "Epoch 280 loss: 0.012\n",
      "Epoch 290 loss: 0.011\n",
      "Epoch 290 loss: 0.011\n",
      "Epoch 290 loss: 0.011\n",
      "Epoch 290 loss: 0.011\n",
      "Epoch 300 loss: 0.011\n",
      "Epoch 300 loss: 0.011\n",
      "Epoch 300 loss: 0.011\n",
      "Epoch 300 loss: 0.011\n",
      "Epoch 310 loss: 0.010\n",
      "Epoch 310 loss: 0.010\n",
      "Epoch 310 loss: 0.010\n",
      "Epoch 310 loss: 0.010\n",
      "Epoch 320 loss: 0.010\n",
      "Epoch 320 loss: 0.010\n",
      "Epoch 320 loss: 0.010\n",
      "Epoch 320 loss: 0.010\n",
      "Epoch 330 loss: 0.009\n",
      "Epoch 330 loss: 0.009\n",
      "Epoch 330 loss: 0.009\n",
      "Epoch 330 loss: 0.009\n",
      "Epoch 340 loss: 0.009\n",
      "Epoch 340 loss: 0.009\n",
      "Epoch 340 loss: 0.009\n",
      "Epoch 340 loss: 0.009\n",
      "Epoch 350 loss: 0.009\n",
      "Epoch 350 loss: 0.009\n",
      "Epoch 350 loss: 0.009\n",
      "Epoch 350 loss: 0.009\n",
      "Epoch 360 loss: 0.008\n",
      "Epoch 360 loss: 0.008\n",
      "Epoch 360 loss: 0.008\n",
      "Epoch 360 loss: 0.008\n",
      "Epoch 370 loss: 0.008\n",
      "Epoch 370 loss: 0.008\n",
      "Epoch 370 loss: 0.008\n",
      "Epoch 370 loss: 0.008\n",
      "Epoch 380 loss: 0.008\n",
      "Epoch 380 loss: 0.008\n",
      "Epoch 380 loss: 0.008\n",
      "Epoch 380 loss: 0.008\n",
      "Epoch 390 loss: 0.008\n",
      "Epoch 390 loss: 0.008\n",
      "Epoch 390 loss: 0.008\n",
      "Epoch 390 loss: 0.008\n",
      "Epoch 400 loss: 0.007\n",
      "Epoch 400 loss: 0.007\n",
      "Epoch 400 loss: 0.007\n",
      "Epoch 400 loss: 0.007\n",
      "Epoch 410 loss: 0.007\n",
      "Epoch 410 loss: 0.007\n",
      "Epoch 410 loss: 0.007\n",
      "Epoch 410 loss: 0.007\n",
      "Epoch 420 loss: 0.007\n",
      "Epoch 420 loss: 0.007\n",
      "Epoch 420 loss: 0.007\n",
      "Epoch 420 loss: 0.007\n",
      "Epoch 430 loss: 0.007\n",
      "Epoch 430 loss: 0.007\n",
      "Epoch 430 loss: 0.007\n",
      "Epoch 430 loss: 0.007\n",
      "Epoch 440 loss: 0.006\n",
      "Epoch 440 loss: 0.006\n",
      "Epoch 440 loss: 0.006\n",
      "Epoch 440 loss: 0.006\n",
      "Epoch 450 loss: 0.006\n",
      "Epoch 450 loss: 0.006\n",
      "Epoch 450 loss: 0.006\n",
      "Epoch 450 loss: 0.006\n",
      "Epoch 460 loss: 0.006\n",
      "Epoch 460 loss: 0.006\n",
      "Epoch 460 loss: 0.006\n",
      "Epoch 460 loss: 0.006\n",
      "Epoch 470 loss: 0.006\n",
      "Epoch 470 loss: 0.006\n",
      "Epoch 470 loss: 0.006\n",
      "Epoch 470 loss: 0.006\n",
      "Epoch 480 loss: 0.006\n",
      "Epoch 480 loss: 0.006\n",
      "Epoch 480 loss: 0.006\n",
      "Epoch 480 loss: 0.006\n",
      "Epoch 490 loss: 0.006\n",
      "Epoch 490 loss: 0.006\n",
      "Epoch 490 loss: 0.006\n",
      "Epoch 490 loss: 0.006\n",
      "Epoch 500 loss: 0.006\n",
      "Epoch 500 loss: 0.006\n",
      "Epoch 500 loss: 0.006\n",
      "Epoch 500 loss: 0.006\n",
      "Epoch 510 loss: 0.005\n",
      "Epoch 510 loss: 0.005\n",
      "Epoch 510 loss: 0.005\n",
      "Epoch 510 loss: 0.005\n",
      "Epoch 520 loss: 0.005\n",
      "Epoch 520 loss: 0.005\n",
      "Epoch 520 loss: 0.005\n",
      "Epoch 520 loss: 0.005\n",
      "Epoch 530 loss: 0.005\n",
      "Epoch 530 loss: 0.005\n",
      "Epoch 530 loss: 0.005\n",
      "Epoch 530 loss: 0.005\n",
      "Epoch 540 loss: 0.005\n",
      "Epoch 540 loss: 0.005\n",
      "Epoch 540 loss: 0.005\n",
      "Epoch 540 loss: 0.005\n",
      "Epoch 550 loss: 0.005\n",
      "Epoch 550 loss: 0.005\n",
      "Epoch 550 loss: 0.005\n",
      "Epoch 550 loss: 0.005\n",
      "Epoch 560 loss: 0.005\n",
      "Epoch 560 loss: 0.005\n",
      "Epoch 560 loss: 0.005\n",
      "Epoch 560 loss: 0.005\n",
      "Epoch 570 loss: 0.005\n",
      "Epoch 570 loss: 0.005\n",
      "Epoch 570 loss: 0.005\n",
      "Epoch 570 loss: 0.005\n",
      "Epoch 580 loss: 0.005\n",
      "Epoch 580 loss: 0.005\n",
      "Epoch 580 loss: 0.005\n",
      "Epoch 580 loss: 0.005\n",
      "Epoch 590 loss: 0.005\n",
      "Epoch 590 loss: 0.005\n",
      "Epoch 590 loss: 0.005\n",
      "Epoch 590 loss: 0.005\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 610 loss: 0.004\n",
      "Epoch 610 loss: 0.004\n",
      "Epoch 610 loss: 0.004\n",
      "Epoch 610 loss: 0.004\n",
      "Epoch 620 loss: 0.004\n",
      "Epoch 620 loss: 0.004\n",
      "Epoch 620 loss: 0.004\n",
      "Epoch 620 loss: 0.004\n",
      "Epoch 630 loss: 0.004\n",
      "Epoch 630 loss: 0.004\n",
      "Epoch 630 loss: 0.004\n",
      "Epoch 630 loss: 0.004\n",
      "Epoch 640 loss: 0.004\n",
      "Epoch 640 loss: 0.004\n",
      "Epoch 640 loss: 0.004\n",
      "Epoch 640 loss: 0.004\n",
      "Epoch 650 loss: 0.004\n",
      "Epoch 650 loss: 0.004\n",
      "Epoch 650 loss: 0.004\n",
      "Epoch 650 loss: 0.004\n",
      "Epoch 660 loss: 0.004\n",
      "Epoch 660 loss: 0.004\n",
      "Epoch 660 loss: 0.004\n",
      "Epoch 660 loss: 0.004\n",
      "Epoch 670 loss: 0.004\n",
      "Epoch 670 loss: 0.004\n",
      "Epoch 670 loss: 0.004\n",
      "Epoch 670 loss: 0.004\n",
      "Epoch 680 loss: 0.004\n",
      "Epoch 680 loss: 0.004\n",
      "Epoch 680 loss: 0.004\n",
      "Epoch 680 loss: 0.004\n",
      "Epoch 690 loss: 0.004\n",
      "Epoch 690 loss: 0.004\n",
      "Epoch 690 loss: 0.004\n",
      "Epoch 690 loss: 0.004\n",
      "Epoch 700 loss: 0.004\n",
      "Epoch 700 loss: 0.004\n",
      "Epoch 700 loss: 0.004\n",
      "Epoch 700 loss: 0.004\n",
      "Epoch 710 loss: 0.004\n",
      "Epoch 710 loss: 0.004\n",
      "Epoch 710 loss: 0.004\n",
      "Epoch 710 loss: 0.004\n",
      "Epoch 720 loss: 0.004\n",
      "Epoch 720 loss: 0.004\n",
      "Epoch 720 loss: 0.004\n",
      "Epoch 720 loss: 0.004\n",
      "Epoch 730 loss: 0.003\n",
      "Epoch 730 loss: 0.003\n",
      "Epoch 730 loss: 0.003\n",
      "Epoch 730 loss: 0.003\n",
      "Epoch 740 loss: 0.003\n",
      "Epoch 740 loss: 0.003\n",
      "Epoch 740 loss: 0.003\n",
      "Epoch 740 loss: 0.003\n",
      "Epoch 750 loss: 0.003\n",
      "Epoch 750 loss: 0.003\n",
      "Epoch 750 loss: 0.003\n",
      "Epoch 750 loss: 0.003\n",
      "Epoch 760 loss: 0.003\n",
      "Epoch 760 loss: 0.003\n",
      "Epoch 760 loss: 0.003\n",
      "Epoch 760 loss: 0.003\n",
      "Epoch 770 loss: 0.003\n",
      "Epoch 770 loss: 0.003\n",
      "Epoch 770 loss: 0.003\n",
      "Epoch 770 loss: 0.003\n",
      "Epoch 780 loss: 0.003\n",
      "Epoch 780 loss: 0.003\n",
      "Epoch 780 loss: 0.003\n",
      "Epoch 780 loss: 0.003\n",
      "Epoch 790 loss: 0.003\n",
      "Epoch 790 loss: 0.003\n",
      "Epoch 790 loss: 0.003\n",
      "Epoch 790 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 810 loss: 0.003\n",
      "Epoch 810 loss: 0.003\n",
      "Epoch 810 loss: 0.003\n",
      "Epoch 810 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 830 loss: 0.003\n",
      "Epoch 830 loss: 0.003\n",
      "Epoch 830 loss: 0.003\n",
      "Epoch 830 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 870 loss: 0.003\n",
      "Epoch 870 loss: 0.003\n",
      "Epoch 870 loss: 0.003\n",
      "Epoch 870 loss: 0.003\n",
      "Epoch 880 loss: 0.003\n",
      "Epoch 880 loss: 0.003\n",
      "Epoch 880 loss: 0.003\n",
      "Epoch 880 loss: 0.003\n",
      "Epoch 890 loss: 0.003\n",
      "Epoch 890 loss: 0.003\n",
      "Epoch 890 loss: 0.003\n",
      "Epoch 890 loss: 0.003\n",
      "Epoch 900 loss: 0.003\n",
      "Epoch 900 loss: 0.003\n",
      "Epoch 900 loss: 0.003\n",
      "Epoch 900 loss: 0.003\n",
      "Epoch 910 loss: 0.003\n",
      "Epoch 910 loss: 0.003\n",
      "Epoch 910 loss: 0.003\n",
      "Epoch 910 loss: 0.003\n",
      "Epoch 920 loss: 0.003\n",
      "Epoch 920 loss: 0.003\n",
      "Epoch 920 loss: 0.003\n",
      "Epoch 920 loss: 0.003\n",
      "Epoch 930 loss: 0.003\n",
      "Epoch 930 loss: 0.003\n",
      "Epoch 930 loss: 0.003\n",
      "Epoch 930 loss: 0.003\n",
      "Epoch 940 loss: 0.003\n",
      "Epoch 940 loss: 0.003\n",
      "Epoch 940 loss: 0.003\n",
      "Epoch 940 loss: 0.003\n",
      "Epoch 950 loss: 0.003\n",
      "Epoch 950 loss: 0.003\n",
      "Epoch 950 loss: 0.003\n",
      "Epoch 950 loss: 0.003\n",
      "Epoch 960 loss: 0.003\n",
      "Epoch 960 loss: 0.003\n",
      "Epoch 960 loss: 0.003\n",
      "Epoch 960 loss: 0.003\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1], # Alice\n",
    "    [25, 6], # Bob\n",
    "    [17, 4], # Charlie\n",
    "    [-15, -6], # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1,\n",
    "    0,\n",
    "    0,\n",
    "    1\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily: 0.967\n",
      "Frank: 0.057\n"
     ]
    }
   ],
   "source": [
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2]) # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily))\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
